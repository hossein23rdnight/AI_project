{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **get ready**"
      ],
      "metadata": {
        "id": "plnkP4fA5cW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z4gCVOezwcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c646ca22-7a68-4e93-8d27-b009675d0070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parsivar\n",
            "  Downloading parsivar-0.2.3.1-py3-none-any.whl (18.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from parsivar) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->parsivar) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->parsivar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->parsivar) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->parsivar) (4.66.4)\n",
            "Installing collected packages: parsivar\n",
            "Successfully installed parsivar-0.2.3.1\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flashtext<3.0,>=2.7 (from hazm)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy==1.24.3 (from hazm)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.14.1)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=fdd252463812355c0d3d3230640bb60e2eddb3c7fbc585520b8fdd55bac56281\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: python-crfsuite, flashtext, pybind11, numpy, fasttext-wheel, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.10.0 numpy-1.24.3 pybind11-2.13.1 python-crfsuite-0.9.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "efab29ef89fa4bee9657aef815d32f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.32.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install parsivar\n",
        "\n",
        "!pip install hazm\n",
        "!pip install transformers\n",
        "#!pip install torch\n",
        "!pip install accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "#import torch\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "from hazm import Normalizer as HazmNormalizer, stopwords_list, word_tokenize, Stemmer\n",
        "from parsivar import Normalizer as ParsivarNormalizer, Tokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n"
      ],
      "metadata": {
        "id": "no_zqTsl63Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading data"
      ],
      "metadata": {
        "id": "YIC_LrbM68qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/data/train_data.csv'\n",
        "data = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "16Lqk7cv6_a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = data['tags'].unique()\n",
        "print(unique_tags)"
      ],
      "metadata": {
        "id": "bLoLvnge7Aup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853a872d-9950-4eb7-bee2-8796c6f4fa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['استانها' 'اجتماعی' 'اقتصادی' 'سیاسی' 'فرهنگی' 'بین الملل' 'ورزشی'\n",
            " 'رسانه ها' 'بازار' 'الشرق الأوسط' 'دانش و محیط زیست' 'جهان' 'ایران'\n",
            " 'NRS-Import' 'فرهنگ و هنر' 'آلمان' 'دیدگاه' 'دوره\\u200cهای زبان آلمانی'\n",
            " 'ورزش' 'اقتصاد' 'گوناگون' 'شبکه\\u200cهای اجتماعی' 'دویچه وله'\n",
            " 'Community D' 'آلمانی پیش\\u200cرفته' 'ویدئو > گزارش' 'استان ها > مرکزی'\n",
            " 'استان ها > قم' 'علمی و دانشگاهی > صنفی، فرهنگی'\n",
            " 'اقتصادی > عمران و اشتغال' 'اقتصادی > تولید و تجارت' 'اجتماعی > سلامت'\n",
            " 'اجتماعی > آموزش و پرورش' 'سیاسی > دولت'\n",
            " 'فرهنگی و هنری > گردشگری و میراث' 'اقتصادی > اقتصاد کلان'\n",
            " 'ورزشی > فوتبال، فوتسال' 'سیاسی > سیاست خارجی' 'اجتماعی > حوادث، انتظامی'\n",
            " 'سیاسی > مجلس' 'اجتماعی > محیط زیست' 'سیاسی > اندیشه امام و رهبری'\n",
            " 'اجتماعی > خانواده' 'سیاسی > حقوقی و قضایی' 'سیاسی > سیاست داخلی'\n",
            " 'استان ها > لرستان' 'علمی و دانشگاهی > علم و فناوری ایران'\n",
            " 'استان ها > گیلان' 'اجتماعی > جامعه، شهری' 'سیاسی > دفاعی - امنيتی'\n",
            " 'علمی و دانشگاهی > پژوهش' 'گرافیک > اینفوگرافیک' 'استان ها > خوزستان'\n",
            " 'اقتصادی > انرژی' 'فرهنگی و هنری > سینما و تئاتر' 'استان ها > سمنان'\n",
            " 'استان ها > خراسان رضوی' 'سیاسی > انرژی هسته\\u200e\\u200cای'\n",
            " 'علمی و دانشگاهی > علم و فناوری جهان' 'استان ها > البرز'\n",
            " 'استان ها > تهران' 'علمی و دانشگاهی > جهاد دانشگاهی'\n",
            " 'ورزشی > سایر ورزش\\u200cها' 'استان ها > هرمزگان'\n",
            " 'اقتصادی > ارتباطات و فناوری اطلاعات' 'علمی و دانشگاهی > آموزش'\n",
            " 'ورزشی > کشتی، رزمی' 'بین الملل > فرامنطقه ای'\n",
            " 'بین الملل > آسیا،خاورمیانه' 'استان ها > اصفهان'\n",
            " 'استان ها > چهارمحال و بختیاری' 'فرهنگی و هنری > تجسمی و موسیقی'\n",
            " 'فرهنگی و هنری > دین و اندیشه' 'فرهنگی و هنری > ادبیات و کتاب'\n",
            " 'استان ها > بوشهر' 'فرهنگی و هنری > فرهنگ حماسه' 'فرهنگی و هنری > رسانه'\n",
            " 'استان ها > سیستان و بلوچستان' 'ورزشی > توپ و تور'\n",
            " 'استان ها > کهگیلویه و بویراحمد' 'استان ها > کرمانشاه'\n",
            " 'استان ها > مازندران' 'استان ها > ایلام' 'استان ها > زنجان'\n",
            " 'ورزشی > جهان ورزش' 'استان ها > کرمان' 'استان ها > یزد'\n",
            " 'استان ها > اردبیل' 'ورزشی > ورزش بانوان' 'استان ها > خراسان جنوبی'\n",
            " 'بین الملل > فلسطین' 'ایسنا+ > ایسنا+' 'استان ها > فارس'\n",
            " 'استان ها > گلستان' 'عکس > خبری' 'بایگانی' 'ايران' 'اقتصاد ایران'\n",
            " 'خبرخوان' 'دانش و فناوری' nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags = {\n",
        "    \"اجتماعی\": \"social\",\n",
        "    \"اقتصادی\": \"economics\",\n",
        "    \"استانها\": \"Iran-states\",\n",
        "    \"بین الملل\": \"international\",\n",
        "    \"سیاسی\": \"politics\",\n",
        "    \"ورزشی\": \"scientific-cultural-sports\",\n",
        "    \"جهان\": \"international\",\n",
        "    \"ایران\": \"Iran-states\",\n",
        "    \"اقتصاد ایران\": \"economics\",\n",
        "    \"رسانه ها\": \"scientific-cultural-sports\",\n",
        "    \"بازار\": \"economics\",\n",
        "    \"الشرق الأوسط\": \"international\",\n",
        "    \"دانش و محیط زیست\": \"scientific-cultural-sports\",\n",
        "    \"NRS-Import\": \"international\",\n",
        "    \"فرهنگ و هنر\": \"scientific-cultural-sports\",\n",
        "    \"آلمان\": \"international\",\n",
        "    \"دیدگاه\": \"politics\",\n",
        "    \"دوره‌های زبان آلمانی\": \"scientific-cultural-sports\",\n",
        "    \"ورزش\": \"scientific-cultural-sports\",\n",
        "    \"اقتصاد\": \"economics\",\n",
        "    \"گوناگون\": \"scientific-cultural-sports\",\n",
        "    \"شبکه‌های اجتماعی\": \"scientific-cultural-sports\",\n",
        "    \"دویچه وله\": \"international\",\n",
        "    \"Community D\": \"scientific-cultural-sports\",\n",
        "    \"آلمانی پیش‌رفته\": \"scientific-cultural-sports\",\n",
        "    \"ویدئو > گزارش\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > مرکزی\": \"Iran-states\",\n",
        "    \"استان ها > قم\": \"Iran-states\",\n",
        "    \"علمی و دانشگاهی > صنفی، فرهنگی\": \"scientific-cultural-sports\",\n",
        "    \"اقتصادی > عمران و اشتغال\": \"economics\",\n",
        "    \"اقتصادی > تولید و تجارت\": \"economics\",\n",
        "    \"اجتماعی > سلامت\": \"social\",\n",
        "    \"اجتماعی > آموزش و پرورش\": \"social\",\n",
        "    \"سیاسی > دولت\": \"politics\",\n",
        "    \"فرهنگی و هنری > گردشگری و میراث\": \"scientific-cultural-sports\",\n",
        "    \"اقتصادی > اقتصاد کلان\": \"economics\",\n",
        "    \"ورزشی > فوتبال، فوتسال\": \"scientific-cultural-sports\",\n",
        "    \"سیاسی > سیاست خارجی\": \"politics\",\n",
        "    \"اجتماعی > حوادث، انتظامی\": \"social\",\n",
        "    \"سیاسی > مجلس\": \"politics\",\n",
        "    \"اجتماعی > محیط زیست\": \"social\",\n",
        "    \"سیاسی > اندیشه امام و رهبری\": \"politics\",\n",
        "    \"اجتماعی > خانواده\": \"social\",\n",
        "    \"سیاسی > حقوقی و قضایی\": \"politics\",\n",
        "    \"سیاسی > سیاست داخلی\": \"politics\",\n",
        "    \"استان ها > لرستان\": \"Iran-states\",\n",
        "    \"علمی و دانشگاهی > علم و فناوری ایران\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > گیلان\": \"Iran-states\",\n",
        "    \"اجتماعی > جامعه، شهری\": \"social\",\n",
        "    \"سیاسی > دفاعی - امنيتی\": \"politics\",\n",
        "    \"علمی و دانشگاهی > پژوهش\": \"scientific-cultural-sports\",\n",
        "    \"گرافیک > اینفوگرافیک\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > خوزستان\": \"Iran-states\",\n",
        "    \"اقتصادی > انرژی\": \"economics\",\n",
        "    \"فرهنگی و هنری > سینما و تئاتر\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > سمنان\": \"Iran-states\",\n",
        "    \"استان ها > خراسان رضوی\": \"Iran-states\",\n",
        "    \"سیاسی > انرژی هسته‌ای\": \"politics\",\n",
        "    \"علمی و دانشگاهی > علم و فناوری جهان\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > البرز\": \"Iran-states\",\n",
        "    \"استان ها > تهران\": \"Iran-states\",\n",
        "    \"علمی و دانشگاهی > جهاد دانشگاهی\": \"scientific-cultural-sports\",\n",
        "    \"ورزشی > سایر ورزش‌ها\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > هرمزگان\": \"Iran-states\",\n",
        "    \"اقتصادی > ارتباطات و فناوری اطلاعات\": \"economics\",\n",
        "    \"علمی و دانشگاهی > آموزش\": \"scientific-cultural-sports\",\n",
        "    \"ورزشی > کشتی، رزمی\": \"scientific-cultural-sports\",\n",
        "    \"بین الملل > فرامنطقه ای\": \"international\",\n",
        "    \"بین الملل > آسیا،خاورمیانه\": \"international\",\n",
        "    \"استان ها > اصفهان\": \"Iran-states\",\n",
        "    \"استان ها > چهارمحال و بختیاری\": \"Iran-states\",\n",
        "    \"فرهنگی و هنری > تجسمی و موسیقی\": \"scientific-cultural-sports\",\n",
        "    \"فرهنگی و هنری > دین و اندیشه\": \"scientific-cultural-sports\",\n",
        "    \"فرهنگی و هنری > ادبیات و کتاب\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > بوشهر\": \"Iran-states\",\n",
        "    \"فرهنگی و هنری > فرهنگ حماسه\": \"scientific-cultural-sports\",\n",
        "    \"فرهنگی و هنری > رسانه\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > سیستان و بلوچستان\": \"Iran-states\",\n",
        "    \"ورزشی > توپ و تور\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > کهگیلویه و بویراحمد\": \"Iran-states\",\n",
        "    \"استان ها > کرمانشاه\": \"Iran-states\",\n",
        "    \"استان ها > مازندران\": \"Iran-states\",\n",
        "    \"استان ها > ایلام\": \"Iran-states\",\n",
        "    \"استان ها > زنجان\": \"Iran-states\",\n",
        "    \"ورزشی > جهان ورزش\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > کرمان\": \"Iran-states\",\n",
        "    \"استان ها > یزد\": \"Iran-states\",\n",
        "    \"استان ها > اردبیل\": \"Iran-states\",\n",
        "    \"ورزشی > ورزش بانوان\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > خراسان جنوبی\": \"Iran-states\",\n",
        "    \"بین الملل > فلسطین\": \"international\",\n",
        "    \"ایسنا+ > ایسنا+\": \"scientific-cultural-sports\",\n",
        "    \"استان ها > فارس\": \"Iran-states\",\n",
        "    \"استان ها > گلستان\": \"Iran-states\",\n",
        "    \"عکس > خبری\": \"scientific-cultural-sports\",\n",
        "    \"بایگانی\": \"scientific-cultural-sports\",\n",
        "    \"خبرخوان\": \"scientific-cultural-sports\",\n",
        "    \"دانش و فناوری\": \"scientific-cultural-sports\",\n",
        "    \"فرهنگی\": \"scientific-cultural-sports\"\n",
        "}\n",
        "\n",
        "data['tags'] = data['tags'].replace(tags)\n",
        "\n",
        "data['tags'].fillna('unknown', inplace=True)\n",
        "\n",
        "print(data['tags'].unique())"
      ],
      "metadata": {
        "id": "i97uhNSA68QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b64ec2a-5752-48aa-e0ab-6b16e1bd737d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iran-states' 'social' 'economics' 'politics'\n",
            " 'scientific-cultural-sports' 'international'\n",
            " 'سیاسی > انرژی هسته\\u200e\\u200cای' 'ايران' 'unknown']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/data/train_data.csv'\n",
        "\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "tag_counts = data['tags'].value_counts()\n",
        "\n",
        "for tag, count in tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCqMuyLkTn_0",
        "outputId": "9f731289-827c-460c-f110-82bd8d1d9684"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بایگانی: 1944\n",
            "جهان: 1658\n",
            "ایران: 1188\n",
            "بین الملل: 972\n",
            "سیاسی: 921\n",
            "سیاسی > دولت: 506\n",
            "اجتماعی: 493\n",
            "اقتصادی: 478\n",
            "استانها: 433\n",
            "سیاسی > سیاست خارجی: 418\n",
            "اجتماعی > سلامت: 407\n",
            "آلمان: 310\n",
            "سیاسی > مجلس: 266\n",
            "سیاسی > سیاست داخلی: 261\n",
            "دانش و محیط زیست: 203\n",
            "سیاسی > حقوقی و قضایی: 166\n",
            "اقتصاد: 133\n",
            "فرهنگی: 126\n",
            "سیاسی > دفاعی - امنيتی: 110\n",
            "اقتصادی > اقتصاد کلان: 108\n",
            "اجتماعی > حوادث، انتظامی: 79\n",
            "فرهنگ و هنر: 75\n",
            "ورزشی > فوتبال، فوتسال: 68\n",
            "علمی و دانشگاهی > آموزش: 67\n",
            "اقتصادی > تولید و تجارت: 63\n",
            "اجتماعی > جامعه، شهری: 61\n",
            "اقتصادی > عمران و اشتغال: 49\n",
            "بین الملل > آسیا،خاورمیانه: 47\n",
            "اجتماعی > آموزش و پرورش: 46\n",
            "فرهنگی و هنری > گردشگری و میراث: 42\n",
            "علمی و دانشگاهی > علم و فناوری ایران: 38\n",
            "اقتصادی > انرژی: 37\n",
            "اجتماعی > محیط زیست: 37\n",
            "سیاسی > انرژی هسته‎‌ای: 35\n",
            "اجتماعی > خانواده: 35\n",
            "سیاسی > اندیشه امام و رهبری: 34\n",
            "ورزش: 33\n",
            "اقتصاد ایران: 32\n",
            "دیدگاه: 31\n",
            "بین الملل > فرامنطقه ای: 24\n",
            "علمی و دانشگاهی > علم و فناوری جهان: 23\n",
            "ورزشی: 22\n",
            "خبرخوان: 20\n",
            "ايران: 20\n",
            "استان ها > کهگیلویه و بویراحمد: 18\n",
            "فرهنگی و هنری > تجسمی و موسیقی: 17\n",
            "فرهنگی و هنری > سینما و تئاتر: 17\n",
            "استان ها > خوزستان: 16\n",
            "اقتصادی > ارتباطات و فناوری اطلاعات: 15\n",
            "NRS-Import: 14\n",
            "فرهنگی و هنری > رسانه: 12\n",
            "استان ها > تهران: 12\n",
            "استان ها > مازندران: 12\n",
            "استان ها > هرمزگان: 11\n",
            "فرهنگی و هنری > دین و اندیشه: 10\n",
            "استان ها > گیلان: 9\n",
            "استان ها > کرمانشاه: 9\n",
            "استان ها > اصفهان: 9\n",
            "ورزشی > سایر ورزش‌ها: 9\n",
            "ورزشی > جهان ورزش: 9\n",
            "علمی و دانشگاهی > پژوهش: 8\n",
            "عکس > خبری: 8\n",
            "استان ها > مرکزی: 8\n",
            "فرهنگی و هنری > ادبیات و کتاب: 7\n",
            "علمی و دانشگاهی > صنفی، فرهنگی: 7\n",
            "استان ها > خراسان رضوی: 7\n",
            "رسانه ها: 7\n",
            "دانش و فناوری: 6\n",
            "علمی و دانشگاهی > جهاد دانشگاهی: 6\n",
            "استان ها > قم: 5\n",
            "فرهنگی و هنری > فرهنگ حماسه: 5\n",
            "گرافیک > اینفوگرافیک: 5\n",
            "استان ها > بوشهر: 4\n",
            "ورزشی > کشتی، رزمی: 4\n",
            "استان ها > لرستان: 4\n",
            "استان ها > سیستان و بلوچستان: 4\n",
            "استان ها > کرمان: 3\n",
            "ورزشی > ورزش بانوان: 3\n",
            "استان ها > زنجان: 3\n",
            "استان ها > سمنان: 3\n",
            "استان ها > گلستان: 3\n",
            "ایسنا+ > ایسنا+: 3\n",
            "استان ها > چهارمحال و بختیاری: 3\n",
            "استان ها > البرز: 3\n",
            "گوناگون: 2\n",
            "استان ها > خراسان جنوبی: 2\n",
            "استان ها > فارس: 1\n",
            "بازار: 1\n",
            "الشرق الأوسط: 1\n",
            "شبکه‌های اجتماعی: 1\n",
            "Community D: 1\n",
            "دویچه وله: 1\n",
            "ویدئو > گزارش: 1\n",
            "آلمانی پیش‌رفته: 1\n",
            "دوره‌های زبان آلمانی: 1\n",
            "ورزشی > توپ و تور: 1\n",
            "استان ها > ایلام: 1\n",
            "استان ها > یزد: 1\n",
            "استان ها > اردبیل: 1\n",
            "بین الملل > فلسطین: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_to_drop = ['سیاسی > انرژی هسته‌ای', 'ايران', 'unknown','سیاسی > انرژی هسته\\u200e\\u200cای']\n",
        "\n",
        "data = data[~data['tags'].isin(tags_to_drop)]\n",
        "\n",
        "print(data['tags'].unique())"
      ],
      "metadata": {
        "id": "8SycptKd7EU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f85a6b-97bc-4f6b-8b75-991fc7326dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iran-states' 'social' 'economics' 'politics'\n",
            " 'scientific-cultural-sports' 'international']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = data['tags'].unique()\n",
        "unmapped_tags = [tag for tag in unique_tags if tag not in tags.values()]\n",
        "print(f\"Unmapped tags: {unmapped_tags}\")"
      ],
      "metadata": {
        "id": "PnEAsLAI7ESM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198c12bc-0d5d-4a2c-e9a6-7de81bb04957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unmapped tags: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# add  preprocessing"
      ],
      "metadata": {
        "id": "67hu0Z9W7U_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['title'] + ' ' + data['description']\n",
        "y = data['tags']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "hazm_normalizer = HazmNormalizer()\n",
        "parsivar_normalizer = ParsivarNormalizer()\n",
        "stopwords = set(stopwords_list())\n",
        "stemmer = Stemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = hazm_normalizer.normalize(text)\n",
        "    text = parsivar_normalizer.normalize(text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess_text)\n",
        "X_test = X_test.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "lKcnVbDH7P2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logestic regression with tfidf\n"
      ],
      "metadata": {
        "id": "GFTnvgui7ckr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "logreg_model = LogisticRegression(max_iter=1000, C=10, solver='lbfgs')\n",
        "\n",
        "logreg_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = logreg_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GzctDXyL7vIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0336b686-9648-414d-de55-0119cb67d9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.777016129032258\n",
            "F1 Score: 0.7766655599748348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logestic with word2vec"
      ],
      "metadata": {
        "id": "IXK85z9e7t4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n"
      ],
      "metadata": {
        "id": "xiVX6-xFu1A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "logreg_model.fit(X_train_w2v, y_train)\n",
        "\n",
        "y_pred = logreg_model.predict(X_test_w2v)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8QrygGEc7Pze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Best result in logestic regression**"
      ],
      "metadata": {
        "id": "bwU1dNr28IdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X = data['description']\n",
        "X = data['title'] + ' ' + data['description']\n",
        "\n",
        "y = data['tags']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=100000)\n",
        "log_reg.fit(X_train_vectorized, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test_vectorized)\n",
        "\n",
        "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "log_reg_f1 = f1_score(y_test, y_pred_log_reg, average='weighted')\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {log_reg_accuracy}\")\n",
        "print(f\"Logistic Regression F1 Score: {log_reg_f1}\")\n"
      ],
      "metadata": {
        "id": "NOzGXBrq8WMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ec9616-c9d2-4d23-bcdb-857ae2bd0eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8012096774193549\n",
            "Logistic Regression F1 Score: 0.8015051049514994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use it for test_data.csv"
      ],
      "metadata": {
        "id": "KtjWY4VcUQMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/data/test_data.csv')\n",
        "\n",
        "test_data['text'] = test_data['title'] + ' ' + test_data['description']\n",
        "test_data['text'] = test_data['text']\n",
        "\n",
        "test_vectorized = vectorizer.transform(test_data['text'])\n",
        "\n",
        "test_predictions = log_reg.predict(test_vectorized)\n",
        "\n",
        "test_labels = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "submission_df = pd.DataFrame(test_labels, columns=['prediction'])\n",
        "\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "nLCxsc6pP6DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "______________\n"
      ],
      "metadata": {
        "id": "Z8y2mEh_9Qhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **low accuracy in MLP and LSTM **"
      ],
      "metadata": {
        "id": "ed1kSinW9P__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence.split() for sentence in X_train]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_w2v, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "y_pred = model.predict(X_test_w2v)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "BkI8oawF9YIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten\n",
        "\n",
        "X_train_w2v_reshaped = X_train_w2v.reshape(X_train_w2v.shape[0], X_train_w2v.shape[1], 1)\n",
        "X_test_w2v_reshaped = X_test_w2v.reshape(X_test_w2v.shape[0], X_test_w2v.shape[1], 1)\n",
        "\n",
        "model_cnn_lstm = Sequential()\n",
        "model_cnn_lstm.add(Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(100, 1)))\n",
        "model_cnn_lstm.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn_lstm.add(Dropout(0.2))\n",
        "model_cnn_lstm.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "model_cnn_lstm.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn_lstm.add(Dropout(0.2))\n",
        "model_cnn_lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model_cnn_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_cnn_lstm.add(Dense(32, activation='relu'))\n",
        "model_cnn_lstm.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model_cnn_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_cnn_lstm = model_cnn_lstm.fit(X_train_w2v_reshaped, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "y_pred_cnn_lstm = model_cnn_lstm.predict(X_test_w2v_reshaped)\n",
        "y_pred_classes_cnn_lstm = np.argmax(y_pred_cnn_lstm, axis=1)\n",
        "\n",
        "accuracy_cnn_lstm = accuracy_score(y_test, y_pred_classes_cnn_lstm)\n",
        "f1_cnn_lstm = f1_score(y_test, y_pred_classes_cnn_lstm, average='weighted')\n",
        "\n",
        "print(f\"Accuracy CNN-LSTM: {accuracy_cnn_lstm}\")\n",
        "print(f\"F1 Score CNN-LSTM: {f1_cnn_lstm}\")\n"
      ],
      "metadata": {
        "id": "IsnOtZ3ecmkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BONUS PART _ MLP**"
      ],
      "metadata": {
        "id": "_EE8h2G55Lvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, GlobalMaxPooling1D, LSTM\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/data/train_data.csv')\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.strip().lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "df['description'] = df['description'].apply(clean_text)\n",
        "\n",
        "df['text'] = df['title'] + ' ' + df['description']\n",
        "\n",
        "label_map = {\n",
        "\n",
        "    'اجتماعی': 'social',\n",
        "    'اقتصادی': 'economics',\n",
        "    'استانها': 'Iran-states',\n",
        "    'بین‌الملل': 'International',\n",
        "    'سیاسی': 'politics',\n",
        "    'علمی فرهنگی ورزشی': 'scientific-cultural-sports'\n",
        "}\n",
        "\n",
        "df['label'] = df['tags'].map(label_map)\n",
        "labels = df['label'].unique().tolist()\n",
        "label_dict = {label: i for i, label in enumerate(labels)}\n",
        "df['label'] = df['label'].map(label_dict)\n",
        "\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=max_len)\n",
        "labels = to_categorical(df['label'], num_classes=len(label_dict))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_len))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(label_dict), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "7S-tMT6L5Lc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fe02d1-4d6d-4e57-d1e0-a59ea02160ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "125/125 [==============================] - 4s 22ms/step - loss: 0.7876 - accuracy: 0.8091 - val_loss: 0.6308 - val_accuracy: 0.8073\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 2s 20ms/step - loss: 0.4981 - accuracy: 0.8279 - val_loss: 0.4751 - val_accuracy: 0.8279\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 0.3616 - accuracy: 0.8657 - val_loss: 0.4189 - val_accuracy: 0.8455\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 0.2363 - accuracy: 0.9196 - val_loss: 0.3721 - val_accuracy: 0.8751\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 0.1318 - accuracy: 0.9632 - val_loss: 0.4078 - val_accuracy: 0.8766\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.0658 - accuracy: 0.9851 - val_loss: 0.4665 - val_accuracy: 0.8741\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 0.0272 - accuracy: 0.9959 - val_loss: 0.5170 - val_accuracy: 0.8751\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 4s 30ms/step - loss: 0.0114 - accuracy: 0.9994 - val_loss: 0.5506 - val_accuracy: 0.8726\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.0053 - accuracy: 0.9999 - val_loss: 0.6029 - val_accuracy: 0.8741\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 3s 20ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.6183 - val_accuracy: 0.8716\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.6553 - val_accuracy: 0.8741\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 2s 20ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.6657 - val_accuracy: 0.8731\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.6668 - val_accuracy: 0.8736\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.7341 - val_accuracy: 0.8710\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.6875 - val_accuracy: 0.8700\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 3s 20ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.7665 - val_accuracy: 0.8716\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.7633 - val_accuracy: 0.8710\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 4s 29ms/step - loss: 9.2411e-04 - accuracy: 0.9999 - val_loss: 0.7846 - val_accuracy: 0.8726\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 9.3228e-04 - accuracy: 0.9999 - val_loss: 0.7865 - val_accuracy: 0.8736\n",
            "Epoch 20/30\n",
            "125/125 [==============================] - 6s 51ms/step - loss: 8.1309e-04 - accuracy: 0.9999 - val_loss: 0.8319 - val_accuracy: 0.8700\n",
            "Epoch 21/30\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.8943 - val_accuracy: 0.8700\n",
            "Epoch 22/30\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 8.5976e-04 - accuracy: 0.9999 - val_loss: 0.8226 - val_accuracy: 0.8675\n",
            "Epoch 23/30\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 7.5414e-04 - accuracy: 0.9999 - val_loss: 0.8619 - val_accuracy: 0.8665\n",
            "Epoch 24/30\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.9617 - val_accuracy: 0.8665\n",
            "Epoch 25/30\n",
            "125/125 [==============================] - 5s 37ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.8948 - val_accuracy: 0.8640\n",
            "Epoch 26/30\n",
            "125/125 [==============================] - 4s 30ms/step - loss: 8.0185e-04 - accuracy: 0.9999 - val_loss: 0.8929 - val_accuracy: 0.8675\n",
            "Epoch 27/30\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.9459 - val_accuracy: 0.8675\n",
            "Epoch 28/30\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 6.5142e-04 - accuracy: 0.9999 - val_loss: 0.9289 - val_accuracy: 0.8680\n",
            "Epoch 29/30\n",
            "125/125 [==============================] - 4s 31ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.9490 - val_accuracy: 0.8655\n",
            "Epoch 30/30\n",
            "125/125 [==============================] - 4s 31ms/step - loss: 7.0676e-04 - accuracy: 0.9999 - val_loss: 0.9692 - val_accuracy: 0.8695\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.9353 - accuracy: 0.8704\n",
            "Test Accuracy: 0.8703852295875549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data/test_data.csv')\n",
        "\n",
        "test_df['title'] = test_df['title'].apply(clean_text)\n",
        "test_df['description'] = test_df['description'].apply(clean_text)\n",
        "test_df['text'] = test_df['title'] + ' ' + test_df['description']\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "reverse_label_map = {v: k for k, v in label_dict.items()}\n",
        "predicted_labels = [reverse_label_map[c] for c in predicted_classes]\n",
        "\n",
        "submission_df = pd.DataFrame(predicted_labels, columns=['prediction'])\n",
        "\n",
        "submission_df.to_csv('submission2.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld0XhUxpEI-V",
        "outputId": "c4eba8d6-a358-4966-dd89-a603690d4975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "put number for main class in submission file"
      ],
      "metadata": {
        "id": "n9YsM6FLVACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "class_mapping = {\n",
        "    'social': 0,\n",
        "    'economics': 1,\n",
        "    'Iran-states': 2,\n",
        "    'international': 3,\n",
        "    'politics': 4,\n",
        "    'scientific-cultural-sports': 5\n",
        "}\n",
        "\n",
        "submission_df = pd.read_csv('/content/submission.csv')\n",
        "\n",
        "submission_df['prediction'] = submission_df['prediction'].map(class_mapping)\n",
        "\n",
        "submission_df['prediction'] = submission_df['prediction'].astype(int)\n",
        "\n",
        "submission_df.to_csv('submission_log_numeric.csv', index=False)"
      ],
      "metadata": {
        "id": "gqbwMu1bU_MN"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}